{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicsX/myAutoScripts/blob/main/pileToParquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "aZAKYBs5u7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"monology/pile\", split='train', streaming=True)"
      ],
      "metadata": {
        "id": "q7l_Xc6rxXMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"stanford-crfm/DSIR-filtered-pile-50M\",split='train', streaming=True)"
      ],
      "metadata": {
        "id": "FUY0HPnMjKmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkwlrVrX0F-n",
        "outputId": "3ee42adc-54eb-4083-e450-51460c52fd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'contents': 'Alsatian Cheese Tart\\n\\nFrench Chef Michel Bernard Platz, co-owner with Jose Sanabria of Out of Flower cooking school and specialty catering in Dallas, recommends traditional Alsatian fare. In Alsae, the Yuletide dinner starts with an onion and leek tart, followed by a hearty one-dish creation called Baeckaoffa served with walnut bread. For dessert, try an Alsatian Cheese Tart.\\n\\nADVERTISEMENT\\n\\nADVERTISEMENT\\n\\nADVERTISEMENT\\n\\nABOUT TEXAS HIGHWAYS\\n\\nPublished monthly by the Texas Department of Transportation, Texas Highways, the official travel magazine of Texas, encourages travel to and within the Lone Star State and tells the Texas story to readers around the world.search for them.\"\\n\\nViktor Troshenkov of the Russian Academy of Sciences told the Tass news agency that the fireball could be part of a prolific meteor shower known as the Leonids, which peaks at this time of year. He said he felt Thursday\\'s fireball likely wasn\\'t the sole meteorite but others maybe were not seen due to thick clouds elsewhere.\\n\\nTroshenkov told Tass that meteor showers can be even stronger. The Leonids reach their maximum once every 33 years -- and the last time that happened was in 1998, he said. Amateur astronomers in the Arctic then saw about 1,000 meteors, 40 meteorites and one fireball in just one night.\\n\\nIn 2013, a meteorite streaked across the Russian sky and exploded over the Ural Mountains with the power of an atomic', 'metadata': {'pile_set_name': ['Pile-CC', 'OpenWebText2']}, 'id': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for e in dataset['train']:\n",
        "  if i == 5:\n",
        "    break\n",
        "  i+=1\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeJxnDfL4azC",
        "outputId": "f0eee0af-a085-46f8-a985-62c2c8808e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'this guy  cracks me up \\n---------------------- Forwarded by Drew Fossum/ET&S/Enron on 05/19/2000 \\n09:09 AM ---------------------------\\n\\n05/19/2000 08:29 AM\\nLouis Soldano\\nLouis Soldano\\nLouis Soldano\\n05/19/2000 08:29 AM\\n05/19/2000 08:29 AM\\nTo: Drew Fossum/ET&S/Enron@ENRON\\ncc:  \\n\\nSubject: Re: Emma Caplan  \\n\\nWell, if she\\'s \"very bright, personable, and enthusiastic\" she certainly \\nwon\\'t fit in with us.   \\n\\nI\\'ll follow up with mike and shelly on the person we were thiniking of \\nsplitting for the summer - that may already be a done deal, if not it may \\nmake sense to do something more permanent but we will need to look at \\nbudget...\\n\\ni could probably use some part time help with Bret Reich exiled to Phase IV \\nbut i should have a licensed lawyer for that.\\n\\n\\n\\n\\n   \\n\\t\\n\\t\\n\\tFrom:  Drew Fossum                           05/18/2000 05:50 PM\\n\\t\\n\\nTo: Michael Moran/ET&S/Enron@ENRON, Dorothy McCoppin/FGT/Enron@ENRON, Louis \\nSoldano/ET&S/Enron@ENRON, Britt Davis/Corp/Enron@ENRON, Shelley \\nCorman/ET&S/Enron@ENRON\\ncc: Robert Jones/Corp/Enron@ENRON, Emily Sellers/ET&S/Enron@ENRON \\n\\nSubject: Emma Caplan\\n\\nAt the suggestion of Robert Jones (you remember him--one of the growing \\nlegion of ex-GPG HR representatives), I interviewed a lady named Emma Caplan \\nyesterday.  She is the wife of a guy that Enron Networks brought over to \\nHouston from London.  She is a \"Solicitor\" in the English legal system, and \\ngot her law degree at the University of Wales.  She has not taken an American \\nbar exam but may be able to take DC or New York.  For some reason related to \\npractice of foreign lawyers in TX, she can\\'t take the TX bar until she has \\ngotten a US law degree.  This person is very bright, personable, and \\nenthusiastic.  She is extremely interested in working for Enron, either as a \\nlawyer once she gets her license squared away, or in a paralegal or analyst \\nposition until then. I left a copy of her resume with Emily, and I\\'d ask that \\nshe please forward it to each of you.  If any of you have need for such an \\nindividual or have ideas she should pursue within Enron (or outside Enron), \\nplease call Robert or myself.  Thanks.  DF', 'meta': {'pile_set_name': 'Enron Emails'}, 'domain': 'Enron Emails'}\n",
            "{'text': 'i will just meet you there', 'meta': {'pile_set_name': 'Enron Emails'}, 'domain': 'Enron Emails'}\n",
            "{'text': 'NYMEX Membership Services\\nMay 9, 2001\\nNotice # MS-35\\n\\nNOTICE OF INTENTION TO SWITCH QUALIFYING CLEARING MEMBER\\n\\n Please be advised that effective with the close of business, May 9, 2001,\\nPioneer Futures, Inc., will no longer guarantee the following member(s):\\n\\nRon Benaharon (WAVE-3549)\\n\\n However, the above mentioned will remain a member guaranteed by MBF Clearing \\nCorp.  In connection therewith, notification of any claims against the \\naforementioned individuals arising out of transactions executed on the \\nExchange up to the close of business May 9, 2001, must be submitted to the \\nCorporate Secretary\\x01,s Office within (10) ten days.\\n\\n Members and Member Firms have (10) days from the date of this notice to \\nadvise the Corporate Secretary\\x01,s Office of any claims in accordance with Rule \\n2.51.\\n\\n\\nTERMINATION OF MEMBER FIRM PRIVILEGES\\n\\n Please be advised that effective with the close of business, May 8, 2001, \\nGPZ  Trading LLC., has voluntarily terminated its Member Firm privileges.\\n\\n Inquiries regarding the above should be directed to the Member Services \\nDepartment at (212) 299-2379.\\n\\n\\n\\n__________________________________________________\\nPlease click on the link below to indicate you have received this\\nemail.\\n\\n\"http://208.206.41.61/email/email_log.cfm?useremail=sara.shackleton@enron.com&\\nrefdoc=(MS-35)\"\\n\\nNote: If you click on the above line and nothing happens, please copy\\nthe text between the quotes, open your internet browser,\\npaste it into the web site address and press Return.', 'meta': {'pile_set_name': 'Enron Emails'}, 'domain': 'Enron Emails'}\n",
            "{'text': \"Wow! I bet you're sick of explaining that one to all the newcomers. That's \\nabout what I figured was going on - an overlap of names used by different \\nmembers of the Enron family. \\n\\nThanks so much for taking the time to respond and explain. And thanks for the \\nwelcome!\\n\\nKate\", 'meta': {'pile_set_name': 'Enron Emails'}, 'domain': 'Enron Emails'}\n",
            "{'text': 'We are parking 4,875 dth/day for the 27th-30th on CNG (deal 281565) and we \\nare taking out 478 dth/day for the month of June (deal 281565).  ENA is \\npaying CNG $.04 times the total volume parked.\\n\\nAngie, these volumes may go up.  I think Tenn is still trying to balance the \\nweekend and they are a little long.  Clarissa will let you know.', 'meta': {'pile_set_name': 'Enron Emails'}, 'domain': 'Enron Emails'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = \"Enron Emails\""
      ],
      "metadata": {
        "id": "7QI2knZ2WGx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "  \"ArmelR/the-pile-splitted\",\n",
        "  subset,\n",
        "  num_proc=8\n",
        ")"
      ],
      "metadata": {
        "id": "4uIoyzn9Rc5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = len(dataset['train'])\n",
        "\n",
        "# Calculate the total number of bytes in the loaded dataset\n",
        "total_size_bytes = dataset['train'].data.nbytes\n",
        "total_rows,total_size_bytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlaN4W6RXnzv",
        "outputId": "bb46ae49-545e-4951-8e43-ddcca15818d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(900246, 1680866499)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"parquet_files\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "i4eRljemdZZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of bytes per file (1GB)\n",
        "size_per_file = 1_000_000_000\n",
        "\n",
        "# Calculate the number of rows per file\n",
        "rows_per_file = int((total_rows / total_size_bytes) * size_per_file)\n",
        "rows_per_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UysOoQlzdfu9",
        "outputId": "ad94e861-18e3-4283-dde7-7537d43f434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "535584"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ZUnryUTae5hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store the Parquet files\n",
        "os.makedirs(\"parquet_files\", exist_ok=True)\n",
        "\n",
        "# Calculate the total number of rows\n",
        "total_rows = len(dataset['train'])\n",
        "\n",
        "# Calculate the total number of bytes in the loaded dataset\n",
        "total_size_bytes = dataset['train'].data.nbytes\n",
        "\n",
        "# Number of bytes per file (1GB)\n",
        "size_per_file = 1_000_000_000\n",
        "\n",
        "# Calculate the number of rows per file\n",
        "rows_per_file = int((total_rows / total_size_bytes) * size_per_file)\n",
        "\n",
        "# Exporting to multiple Parquet files\n",
        "start_idx = 0\n",
        "file_idx = 0\n",
        "while start_idx < total_rows:\n",
        "    end_idx = min(start_idx + rows_per_file, total_rows)\n",
        "\n",
        "    # Select the subset of data for the current file\n",
        "    subset_data = dataset['train'].select(range(start_idx, end_idx))\n",
        "\n",
        "    # Convert the dataset to a Table\n",
        "    subset_table = pa.Table.from_pandas(subset_data.data.to_pandas())\n",
        "\n",
        "    # Save to Parquet\n",
        "    pq.write_table(subset_table, f\"parquet_files/dataset_{file_idx}.parquet\")\n",
        "\n",
        "    # Update indices\n",
        "    start_idx = end_idx\n",
        "    file_idx += 1\n"
      ],
      "metadata": {
        "id": "EnsHWGq3dpF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_sources = [\n",
        "    \"NIH ExPorter\",\n",
        "    \"PhilPapers\",\n",
        "    \"Enron Emails\"\n",
        "    ]\n",
        "\n",
        "os.makedirs(\"parquet_files\", exist_ok=True)\n",
        "\n",
        "for subset in tqdm(data_sources, desc=\"Data Sources\"):\n",
        "    print(f\"Processing {subset}...\")\n",
        "\n",
        "    folder_name = subset.replace(\" \", \"_\")\n",
        "\n",
        "    dataset = load_dataset(\"ArmelR/the-pile-splitted\", subset, num_proc=8)\n",
        "\n",
        "    concatenated_dataset = concatenate_datasets([dataset['train'], dataset['test']])\n",
        "\n",
        "    os.makedirs(f\"parquet_files/{folder_name}\", exist_ok=True)\n",
        "\n",
        "    total_rows = len(concatenated_dataset)\n",
        "    total_size_bytes = concatenated_dataset.data.nbytes\n",
        "    size_per_file = 1_000_000_000\n",
        "    rows_per_file = int((total_rows / total_size_bytes) * size_per_file)\n",
        "\n",
        "    start_idx = 0\n",
        "    file_idx = 0\n",
        "    pbar = tqdm(total=total_rows, desc=f\"Saving {subset}\")\n",
        "    while start_idx < total_rows:\n",
        "        end_idx = min(start_idx + rows_per_file, total_rows)\n",
        "\n",
        "        subset_data = concatenated_dataset.select(range(start_idx, end_idx))\n",
        "\n",
        "        subset_table = pa.Table.from_pandas(subset_data.data.to_pandas())\n",
        "\n",
        "        pq.write_table(subset_table, f\"parquet_files/{folder_name}/dataset_{file_idx}.parquet\")\n",
        "\n",
        "        pbar.update(end_idx - start_idx)\n",
        "\n",
        "        start_idx = end_idx\n",
        "        file_idx += 1\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"Exported {subset} to {file_idx} Parquet files.\")\n",
        "\n",
        "    del dataset\n",
        "    del concatenated_dataset\n",
        "    del subset_data\n",
        "    del subset_table\n",
        "\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "enA5Sg_KeWhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"parquet\", data_files=[\"s3://<bucket name>/<data folder>/data-parquet\"],\n",
        "    storage_options=fs.storage_options, streaming=True)"
      ],
      "metadata": {
        "id": "wbteP3Jo3BCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for e in dataset['train']:\n",
        "  if i == 5:\n",
        "    break\n",
        "  i+=1\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "Psxgfq3nt5jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yvPhD2kuMsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}